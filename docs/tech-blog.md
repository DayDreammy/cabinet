# Cabinet 技术博客：从产品定义到 Golden Pipeline MVP

> 目标：用最小可行实现（MVP）验证一件事——**“全文语境阅读”是否能显著提升“信任感”和“准确度”**。

---

## 0. 背景：为什么要做 Cabinet

在信息过载与 AI 幻觉并存的时代，“能回答”不是稀缺能力，“**可信**”才是。

Cabinet 的产品定位在 `docs/product-definition-cabinet.md` 里写得很清楚：
- **角色**：首席案头研究员（Chief Desk Researcher）
- **核心价值**：以私人信源为基础的决策辅助与深度导读
- **愿景**：在信息过载与 AI 幻觉的时代，为用户提供一个**绝对可信**、**过程透明**、**还原语境**的思考代理人。它不生产内容，它用你信任的智慧来审视你的难题。

这份定义也直接指出了三个痛点：
1) 纯搜索（关键词匹配）无法理解复杂意图，Recall 低  
2) 切片式 RAG 会丢语境，逻辑断裂  
3) 传统 Chatbot 是黑盒，等待焦虑，输出不可验证  

因此 Cabinet 的核心策略是：
- **引用为中心（Quote-Centric）**：严禁改写，只能引用原文句子，并且能定位到全文位置（高亮溯源）
- **过程透明（Transparent Lab）**：把“检索—阅读—筛选—交付”的过程显式展示出来

---

## 1. MVP 设计：The Golden Pipeline（黄金流水线）

MVP 文档沉淀在 `docs/mvp.md`，最终定稿是“线性三步走”，不引入任何复杂 Agent 框架：

### Step 1：暴力检索（Brute-Force Search）
- 输入：用户 Query
- 动作：对本地 JSON 数据做**加权关键词匹配**
- 目标：保召回率（宁可错杀，不可放过）

### Step 2：并发审阅（Parallel Review）
- 动作：把候选文档并发扔给 LLM 审阅
- Prompt：审核员模式——“是否回答了问题？如果有，摘录原话；如果没有，返回空”
- 目标：用 LLM 的理解能力替代关键词匹配，提高准确率

### Step 3：格式化组装（Assembly）
- 动作：对命中证据排序、分层、组装成最终报告
- 输出：给前端的最终 JSON（含 quote 与可定位 offset）

### “伪装成 Agent”的 UI（Wizard of Oz）
后台是死的 Workflow，前台演得像 Agent：
- 通过 SSE 流式推送日志 + 命中卡片，让用户“看见”系统在并发阅读与筛选（Manus Effect）

---

## 2. 数据：我们在什么上面做检索与阅读

当前 MVP 的数据来自 `data/ps_2026-01-07.json`（约 3355 条），字段结构（见 `docs/mvp.md`）：
- `id / title / question / content / url / publishedAt / updatedAt / proofread`

致谢与版权说明（也写在 `README.md`）：
- 数据来自 `sooon.ai` 的整理
- 来源作者为知乎用户： https://www.zhihu.com/people/kvxjr369f （开放版权与无私奉献，感谢）

MVP 的铁律非常硬：
- 不改写、不总结，只能摘录原句（并且必须是 `content` 的精确子串）
- 摘录必须可溯源：能定位 `quote_start/quote_end` 并在全文高亮

---

## 3. 实现：用最小后端 + 最薄前端跑通端到端

### 3.1 后端：FastAPI + StreamingResponse（SSE）

核心代码在 `main.py`：
- `/stream_research`：黄金流水线主入口（SSE）
- `/extract_keywords`：长问题的关键词/概念抽取（调用 LLM）
- `/doc/{id}`：返回原文（用于前端“点开看全文”）
- `/debug_review`：调试某篇文档的 LLM 输入/输出（排查“明明命中却被判 0 分”）

我们没有引入 `sse_starlette`，而是用 `StreamingResponse` 手工拼 SSE 格式（事件类型 + data 行）。

### 3.2 Step 1：加权关键词检索

在 `search.py` 里做了非常朴素但好用的加权匹配：
- `title` 权重 2.0
- `question` 权重 1.5
- `content` 权重 1.0

并在服务端日志里把关键中间态都打印出来，方便定位“为什么没召回/为什么召回顺序不对”。

### 3.3 Step 2：并发审阅（ThreadPoolExecutor）

在 `main.py` 中通过 `ThreadPoolExecutor` 对候选文档并发调用 LLM：
- 后端实时推 SSE：命中则 `card_found`，未命中则 `log_skip`
- 前端用户看到的是“文档一篇篇被快速阅读并筛掉/锁定”

并发度目前通过参数 `max_workers` 控制，默认允许到 100（这是为了压测与快速迭代；真正开放给外部用户时需要限流与队列）。

### 3.4 引用精确匹配：让“引用”真的能高亮回原文

实际迭代里，最容易踩坑的是：
- LLM 返回的 quote 往往含有不同的引号样式（中英文引号）、空白差异、换行差异
- 导致“看起来一样”，但在 `content.find(quote)` 里找不到，从而 `score` 被打成 0

这块能力在 `review.py` 中逐步增强：
- 对引号（curly quotes）与空白做规范化
- 多策略匹配（精确/trim/normalize/压缩空白等）
- 通过 `/debug_review` 把 payload/response/parsed 都暴露出来，方便端到端排错

### 3.5 多路召回：长问题先抽取概念，再多路检索合并

当用户输入是“比较复杂的一段话”时，我们会：
1) 先分词判断是否为“长问题”
2) 调用 LLM 提取 3–10 个搜索概念（不仅是表面词）
3) 对每个 term 各自检索
4) 合并候选（按累计 `search_score` 排序；并保留 `search_hits` 作为“被多少路召回”的信号）

这一步把产品定义里“语义膨胀与多路召回”的方向，提前用最小实现落地了一部分。

### 3.6 结果分层 + 纯文本报告：让用户能“立刻拿去用”

为了降低用户的“消费成本”，我们把结果做了分层，并且额外生成了一个“可直接复制去写回答”的纯文本版本：
- 阈值常量在 `main.py`：
  - `SCORE_MUST = 10`（推荐先阅读）
  - `SCORE_RECOMMEND = 8`（推荐阅读）
  - `SCORE_MIN = 5`（扩展阅读，最多 10 条）
- `text_report`：每条按 `# title`、quote、url 三行输出，便于复制粘贴

---

## 4. 前端：一页 HTML，把“可信感”做出来

前端在 `public/index.html`，定位是“给人直接测试的 UI”，重点有三点：
1) **SSE 流式 Logs**：把后端关键过程实时展示出来  
2) **Results 卡片**：按 score 排序，打上“推荐先阅读/推荐阅读/扩展阅读”标签  
3) **点开看原文 + 高亮引用**：点击卡片 View，打开全文并高亮引用段落（可溯源）  

为了不影响普通用户体验，调试信息也做了收敛：
- Candidates (Debug) 默认折叠，需要手动展开

---

## 5. 我们是怎么迭代到“能用且可信”的

从实现路径上看，我们把系统从“能跑”逐步推进到“可信”，关键迭代点大致是：
- 先跑通 Golden Pipeline 的端到端（检索→并发阅读→SSE→卡片）
- 加入服务端详细日志，把中间态全部打印出来（排错效率提升一个数量级）
- 引用匹配变得更鲁棒：修复“LLM 明明返回了句子，但定位失败导致 0 分”
- 加入 `/doc/{id}` 与前端高亮，兑现“还原语境”
- 加入 `/debug_review`，把“模型说了什么”变成可检验事实
- 长问题关键词抽取 + 多路检索合并，提升 Recall
- 结果分层与 text_report，让交付更直接、更可消费

---

## 6. 最终实现效果（MVP）

现在这个 MVP 的体验闭环是：
1) 用户输入问题（支持多行）
2) UI 开始播放“研究过程”（日志滚动、并发命中卡片）
3) 用户得到 5+ 条可溯源的原句证据，并能点开原文高亮定位
4) 用户还能一键复制“纯文本答案”，直接用于外部平台的回答草稿

这基本完成了 MVP 的验证目标：用“全文语境 + 可溯源引用 + 过程透明”来换取信任。

---

## 7. 下一步方向（从 MVP 到可公开服务）

如果要把它真正开放给更多人使用，接下来的方向非常清晰：

### 7.1 产品方向
- 真正的 Split View 导读仪表盘：左侧证据流，右侧原文阅读器联动滚动
- 项目化（Project-based）持久化：一次研究形成“报告”，可复看、可分享
- 更强的“语义膨胀”：不仅抽关键词，还能产生反义/对立概念与约束条件

### 7.2 工程方向
- 限流/鉴权/配额：避免并发打穿模型与成本失控
- 队列与任务系统：把并发阅读变成可控的后台任务（并对前端继续 SSE 推进度）
- 观测与可回放：把每次请求的关键过程落盘，支持复现与调参
- 部署拆分：GitHub Pages 只能托管静态前端，后端需要独立部署（并配置 CORS）

---

## 8. 本地运行（给想复现实验的人）

1) 启动本地 LLM（OpenAI 兼容接口）：`http://127.0.0.1:8000/v1/chat/completions`  
2) 启动后端：
```bash
python -m uvicorn main:app --host 127.0.0.1 --port 8002
```
3) 打开浏览器：`http://127.0.0.1:8002/`

